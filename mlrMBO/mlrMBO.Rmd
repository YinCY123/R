---
title: "mlrMBO"
author: "yincy"
date: "6/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Overview  
The main goal of mlrMBO is to optimize expensive black-box functions by model-based optimization (aka Bayesian optimization) and to provide a unified interface for different optimization tasks and algorithmic MBO variants. Supported are, among other things:  

- Effficient global optimization (EGO) of problems with numerical domain and Kriging as surrogate  
- Using arbitrary regression models from `mlr` as surrogates  
- Built-in parallelization using multi-point proposals  
- Mixed-space optimization with categorical and subordinate parameters, for parameter configuration and tuning  
- Multi-criteria optimization  

This vignette gives a brief overview of the features of `mlrMBO`. A more detailed documentation can be found on: http://mlr-org.gtihub.iomlrMBO/   


# Quickstart  
## Prerequisties   
Installing `mlrMBO` will also install and load dependencies `mlr`, `ParamHelpers`, and `smoof`. For this tutorial, you also need the additional packages `DiceKriging` and `randomForest`.  

```{r, message=FALSE, warning=FALSE}
library(mlrMBO)
library(smoof)
library(ParamHelpers)
library(lhs)
```


## General MBO workflow  
1. Define **objective function** and its parameters using the package `smoof`.  
2. Generate **initial design** (optional).  
3. Define `mlr` learner for **surrogate model** (optional).  
4. Set up a **MBO control** object.  
5. Start the optimization with `mbo()`.  

As a simple example we minimize a cosine-like function with an initial design of 5 points and 10 sequential MBO iterations. Thus, the optimizer is allowed 15 evaluations of the objective function in total to approximate the optimum.  

## Objective Function  
Instead of manually defining the objective, we use the `smoof` package which offers many toy and benchmark functions for optimization.  
```{r}
obj.fun <- makeCosineMixtureFunction(dimensions = 1)
obj.fun <- convertToMinimization(fn = obj.fun)
print(obj.fun)
```

```{r}
ggplot2::autoplot(obj.fun)
```

You are not limited to these test functions but can define arbitary object functions with *smoof*.  
```{r}
makeSingleObjectiveFunction(
    name = "my_sphere", 
    fn = function(x){
        sum(x*x) + 7
    }, 
    par.set = makeParamSet(
        makeNumericVectorParam("x", len = 2L, lower = -5, upper = 5)
    ), 
    minimize = TRUE
)
```

Check `?smoof::makeSingleObjectiveFunction` for futher details.  

## Initial Design  
Before MBO can really start, it needs a set of already evaluated points - the *inital design*, as we have to initially lean our machine learning regression model to propose new points for evaluation. If no design is given (i.e `design = TRUE`), `mbo()` will will use a *Maximin Latin Hypercube* `lhs::maximinLHS()` design with `n = 4 * getNumberOfParameters(obj.fun)` points. If the design does not include function outcomes `mbo()` will evaluate the design first before starting with the MBO algprithm. In this example we generate our own design.  
```{r}
des <- generateDesign(n = 5, 
                      par.set = getParamSet(obj.fun), 
                      fun = lhs::randomLHS)
```

we will also precalculate the results  
```{r}
des$y <- apply(des, 1, obj.fun)
```

*Note: mlrMBO* uses `y` as a default name for the outcome of the objective function. This can be changed in the control object.  

## Surrogate Model  
We decide to use Kriging as our surrogate model because it has proven to be quite effective for numerical domains. 




















