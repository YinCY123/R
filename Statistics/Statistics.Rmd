    ---
title: "Statistics"
author: "yincy"
date: "1/7/2020"
output: 
    prettydoc::html_pretty:
        toc: true 
        toc_depth: 2
        display_code: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Chapter 2

**Course data package**: ISLR

## Histry of Statistical Learning  

- linear regression  
- linear discriminant analysis  
- logistic regression  
- generalized linear models  
- classification and regression trees  
- generalized additive models  


## Why Estimate f?  

- Prediction  
- Inference  


## How Do we Estimate f?  

- Parametric Methods  
- Non-parametric Methods  


## The Trad-Off Between Prediction Accuracy and Model Interpretability  
```{r}
knitr::include_graphics(path = "./images/Flexibility - Interpretability.PNG")
```


## The Bias-Variance Trad-Off   
**Variance**: refers to the amount by which f would change if we estimated it using a different training data set. In general, more flexible statistical methods have higher variance.  

**Bias**: refers to the error that is introduced by approximating a real-life problem, which may be extremely complicated, by a much simpler model. For example, linear regression assumes that there is a linear relationship between Y and X~1~, X~2~, ..., X~p~. It is unlikely that any real-life problem truly has such a simple linear relationship, and so performing linear regression will undoubtedly result in some bias in the estimate of f.  

Generally, more flexible methods result in less bias.  

As a general rule, as we use more flexible methods, the variance will increase and the bias will decrease. The relative rate of change of these two quantities determines whether the test MSE increase or decreases.  

```{r}
knitr::include_graphics(path = "./images/Squared bias - variance - MSE.PNG")
```


The challenge lies in finding a method for which both the variance and the squared bias are low. This trad-off is one of the most important recurring themes in Statistics.  


In both the regression and classification settings, choosing the correct level of flexibility is critical to the success of any statistical learning method.  


##### Chapter 2 exercises  

```{r, message=FALSE}
library(ISLR)
library(tidyverse)
library(magrittr)
```


```{r}
data("College")
str(College)
```


```{r}
College[1:5, 1:5]
```

```{r}
summary(College)
```

```{r}
pairs(College[, 1:10])
boxplot(College$Outstate, College$Private, names = c('Outstate', "Private"))
```

```{r}
College %>% 
    mutate(Elite = Top10perc > 50) %>% 
    summary()
```



```{r}
data("Auto")
Auto <- na.omit(Auto)
```


```{r}
str(Auto)
```

```{r}
apply(Auto[, -9], 2, range)
```


```{r}
apply(Auto[, -9], 2, mean)
apply(Auto[, -9], 2, var)

apply(Auto[-c(10:85), -9], 2, mean)
apply(Auto[-c(10:85), -9], 2, var)
```


```{r}
dim(Auto)
```


```{r}
auto_cor <- cor(Auto[, -9])
pheatmap::pheatmap(auto_cor)
```


```{r}
par(mfrow = c(2, 2))
plot(Auto$weight, Auto$horsepower)
plot(Auto$weight, Auto$displacement)
plot(Auto$weight, Auto$mpg)
plot(Auto$weight, Auto$year)
```

```{r, message=FALSE}
library(MASS)
```


```{r}
data("Boston")
Boston
```

```{r}
dim(Boston)
```


```{r}
pairs(Boston[, 1:4])
```

```{r}
b_cor <- cor(Boston)
pheatmap::pheatmap(b_cor)
```

```{r}
par(mfrow = c(1, 2))
plot(Boston$crim, Boston$rad)
plot(Boston$crim, Boston$tax)
```


```{r}
table(Boston$chas)
```


```{r}
median(Boston$ptratio)
```




# Chapter 3 Linear Regression  
```{r}
data("Credit")

Credit_dum <- Credit
Credit_dum$gender_dum <- ifelse(Credit$Gender == "Female", 1, -1)

log_reg <- lm(Credit_dum$Balance ~ Credit_dum$gender_dum)
```


```{r}
data("Auto")
colnames(Auto)
mix_reg <- lm(Auto$acceleration ~ Auto$cylinders + Auto$horsepower + Auto$weight)
summary(mix_reg)
```


### Simple Linear Regression  

```{r}
library(MASS)
library(ISLR)
```


```{r}
data("Boston")
colnames(Boston)
```


```{r}
lm.fit <- lm(medv ~ lstat, data = Boston)
```

```{r}
summary(lm.fit)
```

```{r}
names(lm.fit)
```


```{r}
predict(lm.fit, data.frame(lstat = c(5, 10, 15)), interval = "confidence") # or prediction
```


```{r}
attach(Boston)
plot(lstat, medv)
abline(lm.fit, col = "red")
detach(Boston)
```

```{r}
par(mfrow = c(2, 2))
plot(lm.fit)
```


```{r}
par(mfrow = c(1,2))
plot(predict(lm.fit), residuals(lm.fit))
plot(predict(lm.fit), rstudent(lm.fit))
```

leverage statistics can be computed for any number of predictors using the `hatvalues()` function.  
```{r}
plot(hatvalues(lm.fit), main = "top 10 leverages")
points(order(hatvalues(lm.fit), decreasing = T)[1:10], 
       hatvalues(lm.fit)[order(hatvalues(lm.fit), decreasing = T)[1:10]], 
       col = "red", pch = 19)
```



### Multiple Linear Regression  
```{r}
lm.fit <- lm(medv ~ lstat + age, data = Boston)
summary(lm.fit)
```

```{r}
lm.fit <- lm(medv ~ ., data = Boston)
summary(lm.fit)
```

```{r}
names(lm.fit)
summary(lm.fit) %>% names()
summary(lm.fit)$r.squared
summary(lm.fit)$sigma
```


```{r}
library(car)
vif(lm.fit) %>% sort(decreasing = T)
```


regression on all variables but age  
```{r}
lm.fit <- lm(medv ~ . - age, data = Boston)
summary(lm.fit)
```

alternatively `update()` function can be used  
```{r}
lm.fit1 <- update(lm.fit, ~. - age)
summary(lm.fit1)
```



### Interaction Terms  
The syntax `lstat:black` tells R to include an interaction term between `lstat` and `black`.  

The syntax `lstat*age` simultaneously includes `lstat`, `age` and the interaction term `lstat X age` as predictors, it is a shorthand for `lstat + age + lstat:age`.  

```{r}
summary(lm(medv ~ lstat*age, data = Boston))
```



### Non-linear Transformations of the Predictors  
The `lm()` function can also accommodate non-linear transformations of the predictors.  

```{r}
lm.fit2 <- lm(medv ~ lstat +I(lstat ^ 2), data = Boston)
summary(lm.fit2)
```

```{r}
par(mfrow = c(2,2))
plot(lm.fit2)
```


```{r}
lm.fit <- lm(medv ~ lstat)
anova(lm.fit, lm.fit2)
```


```{r}
lm.fit5 <- lm(medv ~ poly(lstat, 5))
summary(lm.fit5)
```

```{r}
par(mfrow = c(2,2))
plot(lm.fit5)
```



### Qualitative Predictors  
```{r}
data("Carseats")
colnames(Carseats)
```

```{r}
lm.fit <- lm(Sales ~ . + Income:Advertising + Price:Age, data = Carseats)
summary(lm.fit)
```


```{r}
contrasts(Carseats$ShelveLoc)
```
```{r}
unique(Carseats$ShelveLoc)
```



#### Exercises  
```{r}
library(ISLR)
data("Auto")
```

```{r}
lm.fit <- lm(mpg ~ horsepower, data = Auto)
```

```{r}
summary(lm.fit)
```



```{r}
predict(lm.fit, data.frame(horsepower = c(98)), interval = "confidence")
predict(lm.fit, data.frame(horsepower = c(98)), interval = "prediction")
```


```{r}
plot(Auto$horsepower, Auto$mpg)
abline(lm.fit, col = "red")
```


```{r}
par(mfrow = c(2, 2))
plot(lm.fit)
```


```{r}
plot(Auto)
```


```{r}
auto_cor <- cor(Auto[, -9])
```


```{r}
lm.fit <- lm(mpg ~ . - name, data = Auto)
summary(lm.fit)
```

```{r}
par(mfrow = c(2, 2), mar = c(4, 4.5, 2, 2))
plot(lm.fit)
```


```{r}
lm.fit <- lm(mpg ~ displacement + weight + year + origin + year:origin + displacement:horsepower, data = Auto)
```

```{r}
summary(lm.fit)
```


```{r}
lm.fit <- lm(mpg ~ displacement + log(weight) + sqrt(year) + origin + year:origin, data = Auto)
summary(lm.fit)
```


```{r}
data("Carseats")
Carseats
```



```{r}
lm.fit <- lm(Sales ~ Price + Urban + US, data = Carseats)
```

```{r}
summary(lm.fit)
```


```{r}
contrasts(Carseats$Urban)
contrasts(Carseats$US)
```

The equation form 
```
sales = (-0.054459) * Price + (-0.021916) * UrbanYes + (1.200573) * USYes
sales = (-0.054459) * Price + (-0.021916) * UrbanYes
sales = (-0.054459) * Price + (1.200573) * USYes
sales = (-0.054459) * Price
```


```{r}
lm.fit <- lm(Sales ~ Price + US, data = Carseats)
summary(lm.fit)
```

```{r}
par(mfrow = c(2,2))
plot(lm.fit)
```


```{r}
set.seed(1)
x <- rnorm(100)
y <- 2*x + rnorm(100)

# linear model without intercept
lm.fit <- lm(y ~ 0 + x)
summary(lm.fit)
```


```{r}
lm.fit <- lm(x ~ 0 + y)
summary(lm.fit)
```






































